import requests
from bs4 import BeautifulSoup
import pymongo
from django_crontab import Crontab
client = pymongo.MongoClient("mongodb://localhost:27017/")
db = client["real_estate"]
collection = db["properties"]
def scrape_property_details(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    name = soup.find("h1", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    cost = soup.find("div", class_="PriceCard__StyledPrice-sc-1454v74-0").text
    property_type = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    area = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    locality = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    city = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    property_link = url
    return {
        "name": name,
        "cost": cost,
        "property_type": property_type,
        "area": area,
        "locality": locality,
        "city": city,
        "property_link": property_link,
    }
#  Defining the function to schedule the scraping script to run twice daily
def schedule_scraping_script():
    # Creating a crontab object
    crontab = Crontab()
    # Adding a job to the crontab
    crontab.add_job(
        "python scrape_99acres.py",
        minute="0",
        hour="0,12",
        day_of_week="*",
        month_of_year="*",
         
    )
    crontab.start()
# Scrapping the property details from each webpage
for page_number in range(1, 10):
    url = f"https://www.99acres.com/property-in-bangalore-city?page={page_number}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    property_links = [
        a["href"] for a in soup.find_all("a", class_="Heading-StyledHeading-sc-10o2g5t-0")
    ]
    for property_link in property_links:
        property_details = scrape_property_details(property_link)
        collection.insert_one(property_details)
schedule_scraping_script()
